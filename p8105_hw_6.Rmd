---
title: "P8105 Homework 6 - UNI: gvs2113"
output: github_document
date: "2023-12-01"
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

set.seed(1)
```

## Problem 1 
**Load in the data from github link:** 
```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_html = read_csv(url)
```

**Creating a `city_state` variable and performing instructed data cleaning steps.**
```{r}
homicide_df = 
  homicide_html |> 
  mutate(
    city_state = paste(city, state, sep = ", "),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1))|> 
  filter(!(city_state %in% c("Dallas, TX", "Pheonix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  mutate(
    victim_age = replace(victim_age, victim_age == "Unknown", NA),
    victim_age = as.numeric(victim_age)
  )
```

**Baltimore, MD logisitic regression**
```{r}
baltimore = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD")

b_fit = glm(resolution ~ victim_age + victim_sex + victim_race, data = baltimore) |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_upper, OR_CI_lower) 

b_fit |> 
  knitr::kable(digits = 3)
```

**Logistic Regression for all cities**
```{r}
all_city = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    model_map = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = df)),
    tidy_model = map(model_map, broom::tidy)) |> 
  select( -model_map, -data) |> 
  unnest(cols = tidy_model) |> 
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |>
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_upper, OR_CI_lower) 

all_city |> 
  head(10) |> 
  knitr::kable(digits = 3)
```

**Plot for OR and CI for each city**
```{r}
all_city |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) + 
  labs(
    title = "Odds Ratios and Confidence Intervals for each City",
    x = "City, State",
    y = "Odds Ratio"
  )
```

The plot above shows the estimated odds ratio and the 95% confidence interval associated with the odds of solving homicides for male victims compared to female victims for all cities contained in the dataset. The majority of the city's odds ratios are less than 1. The city with the lowest odds ratio is New York, NY and the city with the highest odds ratio is Albuquerque, NM. Albuquerque also has the widest confidence interval out of all data points. The narrowest interval is for Chicago, IL. 

## Problem 2 
**Load in the data:** 
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

**Bootstrapping and Data Wrangling**
```{r}
weather_boot = 
  weather_df |> 
  modelr::bootstrap(n = 5000) |>  
  mutate( 
    model = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map (model, broom::tidy),
    results_2 = map(model, broom::glance)) |> 
  unnest(results) |> 
  select (.id, term, estimate, results_2) |> 
  unnest(results_2) |> 
  select(.id, term, estimate, r.squared) 

weather_boot_pivot = 
  weather_boot |> 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) |> 
  mutate(log_calc = log(tmin * prcp))
```

**Plot of distribution of R squared estimate**
```{r}
ggplot(data = weather_boot_pivot, aes(x=r.squared)) + geom_density() + 
  labs(
    title = "Distribution of R Squared Estimate",
    x = "R Squared Estimate",
    y = "Count"
  )
```

The above plot is meant to show the distribution of the R squared estimates after each bootstrap. It has a normal shape with a slight left skew. 

**Plot of distribution of log(B1 x B2) estimate**
```{r}
ggplot(data = weather_boot_pivot, aes(x=log_calc)) + geom_density() + 
  labs(
    title = "Distribution of log(B1 * B2) Estimate",
    x = "R Squared Estimate",
    y = "Count"
  )
```

The above plot is meant to show the distribution of the log (B1* B2) calculations estimates after each bootstrap. It has a more narrow peak shape with a left skew. 

**Confidence Intervals**
```{r}
conf_int =   
  weather_boot_pivot |> 
  select(r.squared, log_calc) |> 
  pivot_longer(
    r.squared:log_calc, 
    names_to = "term",
    values_to = "estimate"
  ) |> 
  drop_na() |> 
  group_by(term) |> 
  summarize(
    ci_lower = quantile(estimate, 0.025), 
    ci_upper = quantile(estimate, 0.975))

conf_int |> knitr::kable(digits = 3)
```

Number of NaN values generated when calculating log(B1*B2)
```{r}
n_NaN = weather_boot_pivot |> 
  filter(log_calc == "NaN") |> 
  nrow()
```

There are `r n_NaN` NaN results after calculating the log(B1*B2) estimate. This resulted from having a negative values for the `prcp` or B2 variable after performing the bootstrapped linear regression. 

## Problem 3
**Load in the data:** 
```{r}
birth_data = read_csv("./data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex = case_match(babysex, 
      1 ~ "male", 
      2 ~ "female"), 
    babysex = as.factor(babysex),
    frace = case_match(frace, 
      1 ~ "white",
      2 ~ "black",
      3 ~ "asian",
      4 ~ "puerto_rican",
      8 ~ "other",
      9 ~ "unknown"),
    frace = as.factor(frace),
    malform = case_match(malform, 
      0 ~ "absent", 
      1 ~ "present"), 
    malform =  as.factor(malform),
    mrace = case_match(mrace, 
      1 ~ "white",
      2 ~ "black",
      3 ~ "asian",
      4 ~ "puerto_rican",
      8 ~ "other",
      9 ~ "unknown"),
    mrace = as.factor(mrace)
  )
```

For all data, there is a 0 entered for columns: `pnumlbw` (previous number of low birth weight babies) and `pnumgsa` (number of prior small for gestational age babies).

**Hypothesized regression model**
```{r}
model_0 = lm(bwt ~ momage + delwt, data = birth_data) 

model_0|> 
  broom::tidy() |> 
  select(term, estimate, p.value) |>
  knitr::kable(digits = 3)
```

It is hypothesized that the variables that assess the mother's physical condition at time of delivery may impact the resulting birth weight of the baby. Thus, the above linear regression model assess the influence of the only two variables that are acquired at the time of delivery: the mother's age and weight. 

**Predictors and Residuals** 

```{r}
res_pred_bwd = 
  birth_data |> 
  modelr::add_residuals(model_0) |> 
  modelr::add_predictions(model_0)
  
  
  ggplot(data = res_pred_bwd, aes(x = pred, y = resid)) + geom_point() + 
    geom_hline(yintercept = 0, color = "red") +
  labs(
    title = "Model 0 Residuals vs. Predictions",
    x = "Predictions",
    y = "Residuals"
  )
```

The above plot shows the distribution of residuals vs. prediction values for the hypothesized birthwieght linear regression model (Model 0). There is a large concentration of values around the y=0 intercept line and radiating out in a circular distribution around the prediction value of 3150. There is more spread for negative residuals and one large outlier among the predictions with a value of around 4750. 

 

**Other models to compare** 
```{r}
model_1 = lm(bwt ~ blength + gaweeks, data = birth_data)

model_1|> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

```{r}
model_2 = lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = birth_data)

model_2|> 
  broom::tidy() |> 
  knitr::kable(digits = 3)
```

**Cross Validation**
```{r}
cv_df = 
  crossv_mc(birth_data, 100) 

cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df |> 
  mutate(
    cv_mod_0  = map(train, \(df) lm(bwt ~ momage + delwt, data = df)),
    cv_mod_1  = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    cv_mod_2  = map(train, \(df) lm(bwt ~ bhead + blength + babysex + bhead * blength + bhead * babysex + blength * babysex + bhead * blength * babysex, data = df))) |> 
  mutate(
    rmse_model_0 = map2_dbl(cv_mod_0, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model_1 = map2_dbl(cv_mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_model_2 = map2_dbl(cv_mod_2, test, \(mod, df) rmse(model = mod, data = df)))

cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(
    title = "RMSE Distributions for each model",
    x = "Model Used",
    y = "RMSE"
  )

```

The root mean square error for each model after 5000 bootstraps is pictured above in side-by-side violin plots. The hypothesized model has much larger values and lacks the shape of the other two models. From this, we can conclude that the hypothesized model is less optimal than the other two models.    